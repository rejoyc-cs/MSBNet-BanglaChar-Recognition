{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defaca6c-184a-4d29-ae3d-40bf25116ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CUDA_VISIBLE_DEVICES environment variable to the index of the GPU you want to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Use GPU 0 (change the index as needed)\n",
    "\n",
    "# Now, TensorFlow will only see the GPU you specified\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "#print(\"Visible GPUs:\", gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5dd9b-71ff-463a-b91d-570417bcc59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs\", len(logical_gpus), \"logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Tf will attempt to allocate only as much GPU Memory as needed for runtime allowed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7103e7-6627-4c1f-9588-6ab19a08a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(seconds): \n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds)) \n",
    "\n",
    "\n",
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        from keras import backend as K\n",
    "    except:\n",
    "        from tensorflow.keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    internal_model_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        layer_type = l.__class__.__name__\n",
    "        if layer_type == 'Model':\n",
    "            internal_model_mem_count += get_model_memory_usage(batch_size, l)\n",
    "        single_layer_mem = 1\n",
    "        out_shape = l.output_shape\n",
    "        if type(out_shape) is list:\n",
    "            out_shape = out_shape[0]\n",
    "        for s in out_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in model.trainable_weights])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in model.non_trainable_weights])\n",
    "\n",
    "    number_size = 4.0\n",
    "    if K.floatx() == 'float16':\n",
    "        number_size = 2.0\n",
    "    if K.floatx() == 'float64':\n",
    "        number_size = 8.0\n",
    "\n",
    "    total_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3) + internal_model_mem_count\n",
    "    return gbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'/home/viraat/Downloads/Rejoy/ekush_train_val_test_split/train'\n",
    "test_dir= r'/home/viraat/Downloads/Rejoy/ekush_train_val_test_split/test'\n",
    "val_dir=r'/home/viraat/Downloads/Rejoy/ekush_train_val_test_split/val'\n",
    "\n",
    "\n",
    "save_file_file= r'ekush_multiscale.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27ca31-6c86-4b0c-8c72-c88b89e1127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_h,img_w= (32,32)\n",
    "batch_size=20\n",
    "epochs=100\n",
    "nb_classes=122\n",
    "\n",
    "\n",
    "train_datagen= tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_datagen= tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "val_datagen=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53f43c-e963-4678-b437-a34066bcaa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=(img_h, img_w),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        color_mode=\"rgb\",\n",
    "        shuffle=True)\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "         test_dir,     \n",
    "        target_size=(img_h, img_w),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        color_mode=\"rgb\",\n",
    "        shuffle=False)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "         val_dir,     \n",
    "        target_size=(img_h, img_w),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        color_mode=\"rgb\",\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058f3fe-755d-4562-8169-a5125510596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_h, img_w, 3)\n",
    "\n",
    "input_img = tf.keras.layers.Input(shape=input_shape)\n",
    "conv1 = tf.keras.layers.Conv2D(64, kernel_size=(5, 5),  padding='same', activation='relu')(input_img)\n",
    "conv2 = tf.keras.layers.Conv2D(64, kernel_size=(5, 5),  padding='same', activation='relu')(conv1)\n",
    "conv3 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  padding='same', activation='relu')(conv1)\n",
    "conv4 = tf.keras.layers.Conv2D(64, kernel_size=(1, 1),  padding='same', activation='relu')(conv1)\n",
    "\n",
    "\n",
    "conv5 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  padding='same', activation='relu')(conv2)\n",
    "conv6 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  padding='same', activation='relu')(conv3)\n",
    "conv7 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  padding='same', activation='relu')(conv4)\n",
    "\n",
    "conv8 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv5)\n",
    "conv9 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv6)\n",
    "conv10 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv7)\n",
    "\n",
    "added1 = tf.keras.layers.Add()([conv8,conv9,conv10])\n",
    "added1 = tf.keras.layers.BatchNormalization()(added1)\n",
    "\n",
    "conv11 = tf.keras.layers.Conv2D(64, kernel_size=(5, 5), padding='same', activation='relu')(added1)\n",
    "conv12 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  padding='same', activation='relu')(added1)\n",
    "conv13 = tf.keras.layers.Conv2D(64, kernel_size=(1, 1),  padding='same', activation='relu')(added1)\n",
    "\n",
    "conv14 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv11)\n",
    "conv15 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv12)\n",
    "conv16 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv13)\n",
    "\n",
    "conv17 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv14)\n",
    "conv18 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv15)\n",
    "conv19 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv16)\n",
    "\n",
    "added2 = tf.keras.layers.Add()([conv17,conv18,conv19])\n",
    "added2 = tf.keras.layers.BatchNormalization()(added2)\n",
    "\n",
    "conv20 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(added2)\n",
    "conv21 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(added2)\n",
    "conv22 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(added2)\n",
    "\n",
    "conv23 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv20)\n",
    "conv24 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv21)\n",
    "conv25 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(conv22)\n",
    "\n",
    "concatenated = tf.keras.layers.Concatenate()([conv23,conv24,conv25])\n",
    "concatenated = tf.keras.layers.BatchNormalization()(concatenated)\n",
    "\n",
    "conv26 = tf.keras.layers.Conv2D(128, kernel_size=(5, 5), strides=(2,2),padding='same', activation='relu')(concatenated)\n",
    "conv27 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),  strides=(2,2),padding='same', activation='relu')(concatenated)\n",
    "conv28 = tf.keras.layers.Conv2D(128, kernel_size=(1, 1), strides=(2,2), padding='same', activation='relu')(concatenated)\n",
    "\n",
    "added3 = tf.keras.layers.Add()([conv26,conv27,conv28])\n",
    "\n",
    "conv29 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu')(added3)\n",
    "conv30 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu')(conv29) ##If works, then remove this and recheck\n",
    "conv30 = tf.keras.layers.BatchNormalization()(conv30)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(conv30)\n",
    "\n",
    "\n",
    "fully_connected_1 = tf.keras.layers.Dense(1024, activation='relu')(flatten)\n",
    "fully_connected_2 = tf.keras.layers.Dense(512, activation='relu')(fully_connected_1)\n",
    "fully_connected_2 = tf.keras.layers.Dropout(0.5)(fully_connected_2)\n",
    "fully_connected_3 = tf.keras.layers.Dense(256, activation='relu')(fully_connected_2)\n",
    "output = tf.keras.layers.Dense(122, activation='softmax')(fully_connected_3)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Model(inputs = input_img, outputs=output,name=\"ekush_multiscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e9854-3188-4ed1-8b00-d4442da2780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4060b08-b0ea-4a78-81d1-86ff97058a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.1,\n",
    "                                         patience=5,\n",
    "                                         cooldown=2,\n",
    "                                         min_lr=1e-10,\n",
    "                                         verbose=1)\n",
    "\n",
    "checkpoint =tf.keras.callbacks.ModelCheckpoint(filepath=save_file_file, \n",
    "                            monitor='val_accuracy',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True, \n",
    "                            save_weights_only=False, \n",
    "                            mode='auto',\n",
    "                            save_freq='epoch')\n",
    "\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=25, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da378d-30eb-4b34-8faf-883160535004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TIMING CALL BACK STORING TIME OF EACH EPOCH\n",
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={'log.txt'}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={'log.txt'}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={'log.txt'}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4081a99-7ba4-47e9-bf9a-b5474d45f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [reduce_learning_rate, checkpoint, time_callback, early]\n",
    "optimizer =tf.keras.optimizers.Adam(learning_rate=0.0001,epsilon=1e-08)\n",
    "model.compile( loss='categorical_crossentropy',optimizer= optimizer, metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d0014-0847-4f51-a608-cb1d935f0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "history = model.fit(\n",
    "      train_generator, \n",
    "      epochs=epochs,\n",
    "      validation_data= val_generator,\n",
    "      callbacks=callbacks,\n",
    "      verbose=1)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d15262",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_time_ms = (end - start) * 10**3\n",
    "\n",
    "hours = execution_time_ms // (1000 * 60 * 60)\n",
    "execution_time_ms %= (1000 * 60 * 60)\n",
    "\n",
    "minutes = execution_time_ms // (1000 * 60)\n",
    "execution_time_ms %= (1000 * 60)\n",
    "\n",
    "seconds = execution_time_ms // 1000\n",
    "\n",
    "print(\"The time of execution of above program is: {} hours, {} minutes, and {} seconds.\".format(int(hours), int(minutes), int(seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca600b-63bd-40c9-b30a-bfcddc01d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_generator,verbose=1)\n",
    "pred_vals= np.argmax(prediction,axis=1)\n",
    "true_lebels=test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995241f-f75d-4b1d-a747-4cf4ceefdfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy is: \" + str(accuracy_score(true_lebels,pred_vals)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab9d90-bcc4-4d3c-9d63-aa5097a735fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=confusion_matrix(true_lebels, pred_vals)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2ab95-b52d-4185-a8e7-aab2528a4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_acc) + 1), train_acc, label=\"Training\")\n",
    "plt.plot(range(1, len(val_acc) + 1), val_acc, label=\"Validation\")  # Added comma and label\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Accuracy Plot\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_acc.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training\")\n",
    "plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation\")  # Added comma and label\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Loss Plot\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_loss.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d51b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prec = history.history['precision']\n",
    "val_prec = history.history['val_precision']\n",
    "train_recall = history.history['recall']\n",
    "val_recall = history.history['val_recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9035cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_prec) + 1), train_prec, label=\"Training\")\n",
    "plt.plot(range(1, len(val_prec) + 1), val_prec, label=\"Validation\")  \n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Precision Plot\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_prec.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e236c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_recall) + 1), train_recall, label=\"Training\")\n",
    "plt.plot(range(1, len(val_recall) + 1), val_recall, label=\"Validation\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Recall Plot\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_recall.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(cf, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=False, yticklabels=False)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Without Class Labels)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
